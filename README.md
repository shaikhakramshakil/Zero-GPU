# üöÄ Cloud LLM API - Run Open-Source LLMs on Free Cloud GPUs

Use **Google Colab's free T4 GPU** to run powerful open-source LLMs (Mistral, Qwen) and access them from **VS Code** via a simple API key. No GPU needed locally.

---

## ‚ö° What is This?

This project lets you:
1. **Create a Colab notebook** with vLLM (lightning-fast LLM inference)
2. **Download open-source models** (Mistral-7B, Qwen-7B, etc.)
3. **Expose an API** with authentication
4. **Connect from VS Code** using an API key for code completions and assistance

## üéØ Why?

- **Free GPU**: Colab gives you free T4 GPU (12hr sessions)
- **No Local GPU Required**: Run models without exhausting your hardware
- **Fast Inference**: vLLM is 10x faster than standard implementations
- **IDE Integration**: Use LLMs right in VS Code while coding
- **Open Source**: 100% free and open-source models

## üìä Supported Models

| Model | Size | Speed | Quality | VRAM |
|-------|------|-------|---------|------|
| **Mistral-7B** | 7B | ‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | ~5GB |
| **Qwen-7B** | 7B | ‚ö° Fast | ‚≠ê‚≠ê‚≠ê‚≠ê | ~5GB |
| **Qwen-14B** | 14B | ‚≠ê Medium | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~10GB |

---

## üöÄ Quick Start (5 minutes)

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         VS Code (Local)                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Extension (API Key)             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Press Ctrl+L ‚Üí Get Completion   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ               ‚îÇ                        ‚îÇ
‚îÇ  HTTP Request ‚îÇ API_KEY + Prompt      ‚îÇ
‚îÇ               ‚ñº                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îò
                                        ‚îÇ
                                        ‚îÇ
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ  Google Colab (Free T4 GPU)             ‚îÇ
                ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
                ‚îÇ  ‚îÇ FastAPI Server (Port 8000)         ‚îÇ ‚îÇ
                ‚îÇ  ‚îÇ + vLLM + Mistral/Qwen Model        ‚îÇ ‚îÇ
                ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                ‚îÇ  (Runs up to 12 hours)                  ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Step 1: Setup Colab (10 minutes)

1. **Open Google Colab**: https://colab.research.google.com
2. **Create New Notebook**
3. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí T4 GPU
4. **Run these cells** in order:

#### Cell 1: Install Dependencies

```python
!pip install -q vllm fastapi uvicorn pydantic pyngrok requests

# Check CUDA
import torch
print(f"CUDA Available: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
```

#### Cell 2: Create API Server

```python
import os
import secrets
import json
from datetime import datetime
from fastapi import FastAPI, HTTPException, Header
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from vllm import LLM, SamplingParams
import uvicorn
from typing import Optional

# Create FastAPI app
app = FastAPI(title="Cloud LLM API", version="1.0.0")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Generate API key
API_KEY = secrets.token_hex(16)
print(f"üîë API KEY: {API_KEY}")

# Models
class CompletionRequest(BaseModel):
    prompt: str
    max_tokens: int = 512
    temperature: float = 0.7
    top_p: float = 0.95

class ChatMessage(BaseModel):
    role: str  # "system", "user", or "assistant"
    content: str

class ChatRequest(BaseModel):
    messages: list[ChatMessage]
    max_tokens: int = 512
    temperature: float = 0.7

# Verify API key
def verify_api_key(authorization: Optional[str] = Header(None)):
    if not authorization or not authorization.startswith("Bearer "):
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    token = authorization.replace("Bearer ", "")
    if token != API_KEY:
        raise HTTPException(status_code=401, detail="Invalid API key")
    
    return token

# Load model (will download first time)
print("üì¶ Loading Mistral model... (this takes 2-3 minutes)")
llm = LLM(
    model="mistralai/Mistral-7B-Instruct-v0.2",
    dtype="float16",
    gpu_memory_utilization=0.9,
    max_model_len=4096
)
print("‚úÖ Model loaded!")

# Health check
@app.get("/health")
async def health(api_key: str = Header(None, alias="Authorization")):
    verify_api_key(api_key)
    return {
        "status": "healthy",
        "model": "mistralai/Mistral-7B-Instruct-v0.2",
        "api_version": "1.0.0",
        "timestamp": datetime.now().isoformat()
    }

# Text completion
@app.post("/v1/completions")
async def completions(request: CompletionRequest, authorization: Optional[str] = Header(None)):
    verify_api_key(authorization)
    
    if not request.prompt or len(request.prompt.strip()) == 0:
        raise HTTPException(status_code=400, detail="Prompt cannot be empty")
    
    if request.max_tokens > 2048:
        raise HTTPException(status_code=400, detail="max_tokens cannot exceed 2048")
    
    # Generate completion
    sampling_params = SamplingParams(
        temperature=request.temperature,
        top_p=request.top_p,
        max_tokens=request.max_tokens
    )
    
    outputs = llm.generate([request.prompt], sampling_params)
    completion_text = outputs[0].outputs[0].text
    
    return {
        "id": secrets.token_hex(4),
        "prompt": request.prompt,
        "completion": completion_text,
        "tokens_used": len(completion_text.split()),
        "model": "mistralai/Mistral-7B-Instruct-v0.2",
        "timestamp": datetime.now().isoformat()
    }

# Chat completion
@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest, authorization: Optional[str] = Header(None)):
    verify_api_key(authorization)
    
    # Format messages for model
    prompt = ""
    for msg in request.messages:
        if msg.role == "system":
            prompt += f"[SYSTEM] {msg.content}\n"
        elif msg.role == "user":
            prompt += f"[USER] {msg.content}\n"
        elif msg.role == "assistant":
            prompt += f"[ASSISTANT] {msg.content}\n"
    
    prompt += "[ASSISTANT]"
    
    sampling_params = SamplingParams(
        temperature=request.temperature,
        max_tokens=request.max_tokens
    )
    
    outputs = llm.generate([prompt], sampling_params)
    response_text = outputs[0].outputs[0].text.strip()
    
    return {
        "id": secrets.token_hex(4),
        "object": "chat.completion",
        "model": "mistralai/Mistral-7B-Instruct-v0.2",
        "choices": [
            {
                "message": {
                    "role": "assistant",
                    "content": response_text
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": len(prompt.split()),
            "completion_tokens": len(response_text.split()),
            "total_tokens": len(prompt.split()) + len(response_text.split())
        }
    }

# Models list
@app.get("/v1/models")
async def list_models(authorization: Optional[str] = Header(None)):
    verify_api_key(authorization)
    return {
        "object": "list",
        "data": [
            {
                "id": "mistralai/Mistral-7B-Instruct-v0.2",
                "object": "model",
                "owned_by": "mistralai"
            }
        ]
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

#### Cell 3: Start Server

```python
# Keep this cell running - don't interrupt it!
# Your API is now live on http://localhost:8000

# Save API key to file
api_key_file = {
    "api_key": API_KEY,
    "base_url": "http://localhost:8000",
    "timestamp": datetime.now().isoformat()
}

with open("api_config.json", "w") as f:
    json.dump(api_key_file, f, indent=2)

print("\n" + "="*50)
print("‚úÖ SERVER STARTED!")
print("="*50)
print(f"üîë API Key: {API_KEY}")
print(f"üìç URL: http://localhost:8000")
print("\nTest endpoint:")
print(f'curl -H "Authorization: Bearer {API_KEY}" http://localhost:8000/health')
print("="*50)
print("\nThis cell will keep running. Don't interrupt it!")
print("="*50)
```

#### Cell 4 (Optional): Public URL with ngrok

```python
from pyngrok import ngrok

# Get public URL
public_url = ngrok.connect(8000)
print(f"üåê Public URL: {public_url}")
print(f"\nüìç Use this URL in VS Code: {public_url}")
```

---

### Step 2: Install VS Code Extension (5 minutes)

1. **Open VS Code**
2. Go to `Extensions` (Ctrl+Shift+X)
3. Search for "Cloud LLM" or manually load from `vscode-extension/`
   - Click "Install from VSIX"
   - Select the `.vsix` file from this project
4. **Configure Extension**:
   - Open Command Palette: `Ctrl+Shift+P`
   - Search: `Cloud LLM: Set API Key`
   - Enter your Colab URL: `http://localhost:8000` (or ngrok URL)
   - Paste API key from Colab

### Step 3: Use in VS Code

- **Get Completion**: Press `Ctrl+L` in any file
- **Extension Settings** (Ctrl+,):
  - `cloud-llm.apiUrl` - Your Colab URL
  - `cloud-llm.maxTokens` - Max completion length (default: 512)
  - `cloud-llm.temperature` - Creativity level (0.7 default)

---

## üìö API Reference

### Base URL

```
http://localhost:8000      # Local Colab
https://xxxxx.ngrok.io     # Public (with ngrok)
```

### Authentication

All requests require Bearer token:

```
Authorization: Bearer YOUR_API_KEY
```

### Endpoints

#### 1. Health Check

```bash
GET /health
Authorization: Bearer YOUR_API_KEY

# Response:
{
    "status": "healthy",
    "model": "mistralai/Mistral-7B-Instruct-v0.2",
    "api_version": "1.0.0",
    "timestamp": "2024-01-01T12:00:00"
}
```

#### 2. Text Completion

```bash
POST /v1/completions
Authorization: Bearer YOUR_API_KEY
Content-Type: application/json

{
    "prompt": "def fibonacci(n):",
    "max_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.95
}

# Response:
{
    "id": "a1b2c3d4",
    "prompt": "def fibonacci(n):",
    "completion": "\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)",
    "tokens_used": 25,
    "model": "mistralai/Mistral-7B-Instruct-v0.2",
    "timestamp": "2024-01-01T12:00:00"
}
```

#### 3. Chat Completion

```bash
POST /v1/chat/completions
Authorization: Bearer YOUR_API_KEY
Content-Type: application/json

{
    "messages": [
        {"role": "user", "content": "What is Python?"}
    ],
    "max_tokens": 512,
    "temperature": 0.7
}

# Response:
{
    "id": "a1b2c3d4",
    "object": "chat.completion",
    "model": "mistralai/Mistral-7B-Instruct-v0.2",
    "choices": [{
        "message": {
            "role": "assistant",
            "content": "Python is a high-level, interpreted programming language..."
        },
        "finish_reason": "stop"
    }],
    "usage": {
        "prompt_tokens": 10,
        "completion_tokens": 50,
        "total_tokens": 60
    }
}
```

#### 4. List Models

```bash
GET /v1/models
Authorization: Bearer YOUR_API_KEY

# Response:
{
    "object": "list",
    "data": [{
        "id": "mistralai/Mistral-7B-Instruct-v0.2",
        "object": "model",
        "owned_by": "mistralai"
    }]
}
```

---

## üõ†Ô∏è Parameters

### Temperature
- **0.0**: Deterministic (always same output)
- **0.7**: Balanced (best for coding)
- **1.0**: Normal randomness
- **2.0**: Very creative

### Top-P (Nucleus Sampling)
- **0.95**: Default, good variety
- **0.5**: More focused
- **1.0**: No filtering

### Max Tokens
- **Min**: 1
- **Max**: 2048
- **Default**: 512

---

## üêõ Troubleshooting

### Issue: "Invalid API key"

**Solution**: Verify API key is exactly correct, regenerate if needed

```python
# Generate new key in Colab
import secrets
NEW_KEY = secrets.token_hex(16)
print(f"New API Key: {NEW_KEY}")
```

### Issue: Connection timeout

**Solution**: Check if Colab server is running
- Look at Colab cell output - should say "SERVER STARTED"
- Restart the server cell if needed

### Issue: "Model loading stuck"

**Solution**: Model download takes 2-3 minutes first time
- Wait for "‚úÖ Model loaded!" message
- Check Colab console for download progress

### Issue: Out of memory errors

**Solution**: Try these in Cell 2:

```python
# Use float32 instead of float16 (uses more memory)
llm = LLM(
    model="mistralai/Mistral-7B-Instruct-v0.2",
    dtype="float32",  # Changed from float16
    gpu_memory_utilization=0.8,  # Reduced from 0.9
    max_model_len=2048  # Reduced from 4096
)
```

Or switch to a smaller model:

```python
# Use Qwen-7B instead
llm = LLM(
    model="Qwen/Qwen-7B",
    dtype="float16",
    gpu_memory_utilization=0.85
)
```

### Issue: ngrok URL not working

**Solution**: ngrok free tier URLs expire after 2 hours
- Regenerate new URL in Cell 4
- Update VS Code extension with new URL

### Issue: Slow completions

**Solution**: 
- Model inference takes 5-30 seconds depending on prompt
- This is normal - vLLM is already optimized
- Reduce `max_tokens` to speed up

### Issue: "CUDA out of memory"

**Solution**: Restart Colab runtime
- Click "Runtime" ‚Üí "Disconnect and delete runtime"
- Start fresh from Cell 1

---

## üìä Performance

**Average Response Times** (Mistral-7B on T4):
- Simple completion (20 tokens): ~3-5 seconds
- Medium completion (100 tokens): ~10-15 seconds
- Long completion (512 tokens): ~30-60 seconds

**Tips for Faster Performance**:
1. Keep `max_tokens` as small as needed
2. Use `temperature=0.0` for deterministic mode
3. Keep Colab session active (no interrupts)
4. Use `top_p=0.5` for more focused outputs

---

## üìù Using Different Models

### Switch to Qwen-7B

In Cell 2, change:

```python
# Old (Mistral)
llm = LLM(model="mistralai/Mistral-7B-Instruct-v0.2", ...)

# New (Qwen)
llm = LLM(model="Qwen/Qwen-7B-Chat", ...)
```

### Switch to Qwen-14B

```python
llm = LLM(
    model="Qwen/Qwen-14B-Chat",
    dtype="float16",
    gpu_memory_utilization=0.85,  # Increased VRAM usage
    max_model_len=2048  # Reduced token length
)
```

**Warning**: Qwen-14B uses ~10GB VRAM, may run out on T4

---

## üîå Extension Commands

| Command | Keybinding | Action |
|---------|-----------|--------|
| Cloud LLM: Set API Key | - | Configure API URL and key |
| Cloud LLM: Test Connection | - | Verify connection works |
| Cloud LLM: Get Completion | Ctrl+L | Insert code completion |
| Cloud LLM: Clear Cache | - | Reset configuration |

---

## üìÅ Project Structure

```
cloud-llm-api/
‚îú‚îÄ‚îÄ README.md                          # Complete documentation (this file)
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îÇ
‚îú‚îÄ‚îÄ cloud-notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ server.py                      # FastAPI server code
‚îÇ   ‚îî‚îÄ‚îÄ setup-colab.py                 # Setup script with colors
‚îÇ
‚îú‚îÄ‚îÄ vscode-extension/
‚îÇ   ‚îú‚îÄ‚îÄ package.json                   # Extension manifest
‚îÇ   ‚îú‚îÄ‚îÄ extension.js                   # Main extension code
‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ       ‚îú‚îÄ‚îÄ apiClient.js               # API communication
‚îÇ       ‚îî‚îÄ‚îÄ completionProvider.js      # Completion logic
‚îÇ
‚îî‚îÄ‚îÄ examples/
    ‚îî‚îÄ‚îÄ (legacy files - not used in new architecture)
```

---

## üõ†Ô∏è Tech Stack

- **Cloud Platform**: Google Colab (Free T4 GPU)
- **LLM Framework**: vLLM (10x faster inference)
- **API Server**: FastAPI + Uvicorn
- **IDE Extension**: VS Code TypeScript/JavaScript
- **Models**: Mistral-7B-Instruct, Qwen (via Hugging Face)
- **Public Tunnel**: ngrok (optional)

---

## üìö Code Examples

### Python - Using the API

```python
import requests

API_KEY = "your_api_key_here"
BASE_URL = "http://localhost:8000"

# Get completion
response = requests.post(
    f"{BASE_URL}/v1/completions",
    json={
        "prompt": "def hello():",
        "max_tokens": 100
    },
    headers={"Authorization": f"Bearer {API_KEY}"}
)

print(response.json()["completion"])
```

### JavaScript - Using the API

```javascript
const API_KEY = "your_api_key_here";
const BASE_URL = "http://localhost:8000";

async function getCompletion(prompt) {
    const response = await fetch(`${BASE_URL}/v1/completions`, {
        method: "POST",
        headers: {
            "Authorization": `Bearer ${API_KEY}`,
            "Content-Type": "application/json"
        },
        body: JSON.stringify({
            prompt: prompt,
            max_tokens: 100
        })
    });
    
    const data = await response.json();
    return data.completion;
}

getCompletion("console.log(").then(console.log);
```

### OpenAI Compatibility

Works with OpenAI SDK:

```python
from openai import OpenAI

client = OpenAI(
    api_key="your_api_key",
    base_url="http://localhost:8000"
)

response = client.chat.completions.create(
    model="mistralai/Mistral-7B",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

---

## ‚ö†Ô∏è Important Notes

1. **Colab Session Limit**: 12 hours max per session
   - Set reminder to restart if needed
   - Colab auto-disconnects after 30 min inactivity

2. **GPU Sharing**: You share T4 with others
   - Performance varies (usually 2-5 min per completion)
   - Peak times may be slower

3. **Data Privacy**: Your prompts go through Colab
   - Don't send sensitive data
   - Consider self-hosted option for production

4. **Model Size**: Mistral-7B uses ~5GB VRAM
   - Fits comfortably on T4 (16GB)
   - Qwen-14B (10GB) is risky

5. **API Key Security**:
   - Keep API key private
   - Regenerate if exposed
   - Use ngrok only for trusted networks

---

## ü§ù Support & Issues

**Common Questions**:

**Q: Can I use other models?**
A: Yes! Change the model name in Cell 2. Any Hugging Face model works (if it fits in VRAM).

**Q: Is my code safe?**
A: Your prompts are sent to your own Colab instance. Use ngrok cautiously.

**Q: How long does Colab stay active?**
A: 12 hours max, or 30 minutes of inactivity.

**Q: Can I use GPU for other tasks?**
A: Yes, but completions will slow down. Keep resources separate.

**Q: What about Colab Pro?**
A: Not required - free tier works great. Pro gives better GPUs.

---

## üìù License

MIT License - Use freely in personal and commercial projects

## üéØ Roadmap

- [ ] Support for more models (LLaMA, Code Llama, etc.)
- [ ] Batch request support
- [ ] Token usage tracking
- [ ] Error analytics
- [ ] VS Code extension marketplace release
- [ ] Faster model serving (with TensorRT)
- [ ] Multi-GPU support

---

## üí° Tips & Tricks

1. **Faster Startups**: Keep Colab open between sessions
2. **Better Quality**: Set `temperature=0.3` for code
3. **Longer Context**: Increase `max_model_len` (uses more VRAM)
4. **Budget**: 1000 free requests/day limit is plenty
5. **Batch Processing**: Generate multiple completions at once

---

**Ready to start? Follow the Quick Start section above!**

**Questions?** Open an issue on GitHub.